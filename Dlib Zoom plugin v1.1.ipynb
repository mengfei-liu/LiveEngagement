{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, sys, pickle\n",
    "import numpy as np\n",
    "import dlib\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shape of 68 features into numpy array format\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "    imagePath = './media/nba1.jpg'\n",
    "    pkl_filename = './model/pickle_logist_model.pkl'\n",
    "    faceCascade_filename = '/model/haarcascade_frontalface_default.xml'\n",
    "    modelPath = './model/shape_predictor_68_face_landmarks.dat'\n",
    "    \n",
    "    # Load Pre-trained Engagement Model\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        myLogsticModel = pickle.load(file)\n",
    "        \n",
    "    # Load Pre-trained Face detection Model\n",
    "    # faceCascade = cv2.CascadeClassifier(faceCascade_filename)\n",
    "    \n",
    "    # Load Dlib Face Detector\n",
    "    faceDetector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    # Load Pre-trained Landmark detection Model\n",
    "    shapePredictor = dlib.shape_predictor(modelPath)\n",
    "    \n",
    "    \n",
    "    return myLogsticModel, faceDetector, shapePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeEngagement(image,engagement_model,face_model,landmark_model, noface_detect):\n",
    "    # Load Models\n",
    "    #engagement_model, face_model, landmark_model = load_pretrained_model()\n",
    "    \n",
    "    # Gray Image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect Faces\n",
    "    # faces = face_model.detectMultiScale(gray,scaleFactor=1.3,minNeighbors=3,minSize=(30, 30))\n",
    "    # print(len(faces))\n",
    "    height = gray.shape[0]\n",
    "    frame_resize_scale = float(height)/480\n",
    "    \n",
    "    imageSmall= cv2.resize(gray, None, fx = 1.0/frame_resize_scale, fy = 1.0/frame_resize_scale, interpolation = cv2.INTER_LINEAR)\n",
    "    faces = face_model(imageSmall,0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        noface_detect = 0\n",
    "        # print(f'{len(faces)} faces detected')\n",
    "        \n",
    "        for (i,face) in enumerate(faces):\n",
    "            sub_face_area = dlib.rectangle(int(face.left() * frame_resize_scale),\n",
    "                                           int(face.top() * frame_resize_scale),\n",
    "                                           int(face.right() * frame_resize_scale),\n",
    "                                           int(face.bottom() * frame_resize_scale))\n",
    "            \n",
    "            sub_face = gray[sub_face_area.top():sub_face_area.bottom(),sub_face_area.left():sub_face_area.right()]\n",
    "            \n",
    "            try:\n",
    "                sub_face_resized = cv2.resize(sub_face,(224,224),interpolation = cv2.INTER_AREA)\n",
    "                rect_value = dlib.rectangle(0, 0, 224, 224)\n",
    "\n",
    "                landmarks = landmark_model(sub_face_resized, rect_value)\n",
    "\n",
    "                landmarks = shape_to_np(landmarks)\n",
    "\n",
    "                landmarks_flatten = landmarks.reshape(-1)\n",
    "\n",
    "                landmarks_expanded = np.expand_dims(landmarks_flatten,axis=1).T\n",
    "\n",
    "                face_prediction_result = engagement_model.predict(landmarks_expanded)[0]\n",
    "                \n",
    "                if face_prediction_result == 1:\n",
    "                    enagement_eachface[i] = 0\n",
    "                else:\n",
    "                    enagement_eachface[i] += 1\n",
    "               \n",
    "                if face_prediction_result:\n",
    "                    cv2.rectangle(image, (sub_face_area.left(), sub_face_area.top()), (sub_face_area.right(), sub_face_area.bottom()), (0,255,0), 2)\n",
    "                    cv2.putText(image, f\"Engaged\", (sub_face_area.left(), sub_face_area.top() - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                else:\n",
    "                    cv2.rectangle(image,  (sub_face_area.left(), sub_face_area.top()), (sub_face_area.right(), sub_face_area.bottom()), (0,0,255), 2)\n",
    "                    cv2.putText(image, f\"Not Engaged\", (sub_face_area.left() - 5, sub_face_area.top() - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        noface_detect += 1\n",
    "        # print('no face detected, noface_detect = ',noface_detect)\n",
    "    return image, noface_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(f'./media/1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enagement_eachface =  [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "face1 = []\n",
    "face2 = []\n",
    "face3 = []\n",
    "\n",
    "enagement_eachface = [0,0,0]\n",
    "noface_detect = 0\n",
    "\n",
    "engagement_model, face_model, landmark_model = load_pretrained_model()\n",
    "framesSkipping = 1\n",
    "count = 1\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,720)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,480)\n",
    "cap.set(10,150)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print('fps = ',fps)\n",
    "idle_time = 1 #seconds\n",
    "idle_frame = idle_time * fps\n",
    "\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "#     if (count % framesSkipping == 0):\n",
    "        \n",
    "        \n",
    "#     count += 1\n",
    "\n",
    "    #clear_output(wait=True)    \n",
    "    #print(f\"count {count} % framesSkipping = {count%framesSkipping} \")\n",
    "\n",
    "    image,res = analyzeEngagement(image,engagement_model,face_model,landmark_model, noface_detect)\n",
    "    noface_detect = res\n",
    "    \n",
    "    clear_output(wait=True) \n",
    "    print('enagement_eachface = ',enagement_eachface)\n",
    "    if noface_detect >= idle_frame:\n",
    "        print(f'No face detect for {round(noface_detect/fps,1)} seconds.')\n",
    "        cv2.putText(image, f\"No face detect for {round(noface_detect/fps,1)} seconds.\", \n",
    "                    (int(image.shape[1]/2) - 150, 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "    if enagement_eachface[0] >= idle_frame:\n",
    "        print(f'Face #1 is not engaged for {round(enagement_eachface[0]/fps,1)} seconds.')\n",
    "        cv2.putText(image, f\"Face #1 is not engaged for {round(enagement_eachface[0]/fps,1)} seconds.\", \n",
    "                    (int(image.shape[1]/2) - 150, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    if enagement_eachface[1] >= idle_frame:\n",
    "        print(f'Face #2 is not engaged for {round(enagement_eachface[1]/fps,1)} seconds.')\n",
    "        cv2.putText(image, f\"Face #2 is not engaged for {round(enagement_eachface[1]/fps,1)} seconds.\", \n",
    "                    (int(image.shape[1]/2) - 150, 45), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    if enagement_eachface[2] >= idle_frame:\n",
    "        print(f'Face #3 is not engaged for {round(enagement_eachface[2]/fps,1)} seconds.')\n",
    "        cv2.putText(image, f\"Face #3 is not engaged for {round(enagement_eachface[2]/fps,1)} seconds.\", \n",
    "                    (int(image.shape[1]/2) - 150, 60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Image', image)\n",
    "    \n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComVi",
   "language": "python",
   "name": "comvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
